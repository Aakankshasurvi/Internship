{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0637bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you knowÂ ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_header_tags(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        for tag in header_tags:\n",
    "            print(tag.text.strip())\n",
    "    else:\n",
    "        print(\"Failed to retrieve page:\", response.status_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    \n",
    "    get_header_tags(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ede447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page: 403\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_rated_movies(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "       \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movie_containers = soup.find_all('div', class_='lister-item-content')\n",
    "        \n",
    "        \n",
    "        names = []\n",
    "        ratings = []\n",
    "        years = []\n",
    "        \n",
    "        for container in movie_containers:\n",
    "        \n",
    "            name = container.find('a').text.strip()\n",
    "            names.append(name)\n",
    "            \n",
    "            rating = float(container.find('span', class_='ipl-rating-star__rating').text)\n",
    "            ratings.append(rating)\n",
    "            \n",
    "            year = int(container.find('span', class_='lister-item-year').text.strip('()'))\n",
    "            years.append(year)\n",
    "       \n",
    "        df = pd.DataFrame({'Name': names, 'Rating': ratings, 'Year': years})\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve page:\", response.status_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    url = \"https://www.imdb.com/chart/top\"\n",
    "    \n",
    "    top_rated_movies_df = get_top_rated_movies(url)\n",
    " \n",
    "    print(top_rated_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4855b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "       \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        restaurant_names = []\n",
    "        cuisines = []\n",
    "        locations = []\n",
    "        ratings = []\n",
    "        image_urls = []\n",
    "        \n",
    "        restaurant_containers = soup.find_all('div', class_='restaurant-list-sec')\n",
    "        \n",
    "        \n",
    "        for container in restaurant_containers:\n",
    "            \n",
    "            name = container.find('div', class_='restnt-info cursor-pointer').h3.text.strip()\n",
    "            restaurant_names.append(name)\n",
    "            \n",
    "          \n",
    "            cuisine = container.find('p', class_='double-line-ellipsis').text.strip()\n",
    "            cuisines.append(cuisine)\n",
    "            \n",
    "            \n",
    "            location = container.find('div', class_='restnt-info cursor-pointer').find('p', class_='double-line-ellipsis mb-0').text.strip()\n",
    "            locations.append(location)\n",
    "            \n",
    "            rating = container.find('span', class_='rating-value').text.strip()\n",
    "            ratings.append(rating)\n",
    "            \n",
    "            image_url = container.find('div', class_='restaurant-image').img['src']\n",
    "            image_urls.append(image_url)\n",
    "        \n",
    "        df = pd.DataFrame({'Restaurant Name': restaurant_names, \n",
    "                           'Cuisine': cuisines, \n",
    "                           'Location': locations, \n",
    "                           'Ratings': ratings, \n",
    "                           'Image URL': image_urls})\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve page:\", response.status_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "    \n",
    "    dineout_details_df = scrape_dineout_details(url)\n",
    "    \n",
    "    print(dineout_details_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2217849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page: 404\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_former_finance_ministers(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', class_='table')\n",
    "        names = []\n",
    "        terms_of_office = []\n",
    "    \n",
    "        rows = table.find_all('tr')[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            name = cells[0].text.strip()\n",
    "            term_of_office = cells[1].text.strip()\n",
    "            \n",
    "            names.append(name)\n",
    "            terms_of_office.append(term_of_office)\n",
    "        \n",
    "        df = pd.DataFrame({'Name': names, 'Term of Office': terms_of_office})\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve page:\", response.status_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "    \n",
    "   \n",
    "    finance_ministers_df = scrape_former_finance_ministers(url)\n",
    "    \n",
    "    print(finance_ministers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df203c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
